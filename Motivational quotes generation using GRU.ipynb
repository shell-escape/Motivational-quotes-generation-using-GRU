{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.1"
    },
    "latex_envs": {
      "LaTeX_envs_menu_present": true,
      "autoclose": false,
      "autocomplete": true,
      "bibliofile": "biblio.bib",
      "cite_by": "apalike",
      "current_citInitial": 1,
      "eqLabelWithNumbers": true,
      "eqNumInitial": 1,
      "hotkeys": {
        "equation": "Ctrl-E",
        "itemize": "Ctrl-I"
      },
      "labels_anchors": false,
      "latex_user_defs": false,
      "report_style_numbering": false,
      "user_envs_cfg": false
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": false
    },
    "colab": {
      "name": "4_My. Генерация лозунгов с помощью RNN.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TqhE3ho6_1W-"
      },
      "source": [
        "# Motivational quotes generation using character/BPE GRU\n",
        "### Based on course [\"Нейронные сети и обработка текста\"](https://stepik.org/course/54098/)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XKccNQv8Ecld"
      },
      "source": [
        "### Data: Author quotes dataset ([author_quotes.txt](https://github.com/Samsung-IT-Academy/stepik-dl-nlp/tree/master/datasets)) containig 36166 quotes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ztTfY1QDDTTJ"
      },
      "source": [
        "### Some examples of character GRU model work:\r\n",
        "\r\n",
        " - \"The condition of the buildings are real wrongs of the people.\"\r\n",
        " - \"I am not a real impact on the table of God\"\r\n",
        " - \"I want to lose my life being weird. I'm not that bus up, emotion, and that's what I've been turned.\"\r\n",
        " - \"The real liberation of it is the result of right.\"\r\n",
        " - \"I think the best way to do is to love it all the time.\"\r\n",
        " - \"I don't really care about the truth.\"\r\n",
        " - \"I am not an award-centrance. I'm big and I'm very lucky.\"\r\n",
        "\r\n",
        "with start phrase \"Neural networks\":\r\n",
        " - \"Neural networks are decisivenial, no nature, and we need to struggle detective news we are passionate.\"\r\n",
        " - \"Neural networks are only bread neuronds of their connection; they can only give a character than to go to their projects.\"\r\n",
        " - \"Neural networks is a compliment to a new enemy.\"\r\n",
        " - \"Neural networks have a very common struggle for me.\"\r\n",
        " - \"Neural networks can be a gentleman in the universe.\"\r\n",
        " - \"Neural networks from the present of the earth is the best construction of the pressure of being able to continue to destroy the results of the world.\"\r\n",
        " - \"Neural networks and their opinion is the best thing I love.\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V1Yb5wg9v0gc"
      },
      "source": [
        "## Required libraries, functions and classes:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xOApOY1xFV1H",
        "outputId": "9b857eac-7c5d-446b-cae0-5be998011842"
      },
      "source": [
        "!pip3 install youtokentome --quiet"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[K     |████████████████████████████████| 1.7MB 5.5MB/s \n",
            "\u001b[?25h"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-11-05T18:20:34.854793Z",
          "start_time": "2019-11-05T18:20:34.372865Z"
        },
        "id": "vxLHOfuc_1W_"
      },
      "source": [
        "from google.colab import drive, files\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import youtokentome as yttm\n",
        "\n",
        "import random\n",
        "\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "import datetime\n",
        "\n",
        "from traceback import format_exc\n",
        "\n",
        "from copy import deepcopy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JluedzS-KBVa"
      },
      "source": [
        "def init_random_seed(value=0):\n",
        "    random.seed(value)\n",
        "    np.random.seed(value)\n",
        "    torch.manual_seed(value)\n",
        "    torch.cuda.manual_seed(value)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "\n",
        "init_random_seed()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DA9oE_KOKxwq"
      },
      "source": [
        "def copy_data_to_device(data, device):\n",
        "    if torch.is_tensor(data):\n",
        "        return data.to(device)\n",
        "    elif isinstance(data, (list, tuple)):\n",
        "        return [copy_data_to_device(elem, device) for elem in data]\n",
        "    raise ValueError('Invalid data type {}'.format(type(data)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PL7pt4XK2RPh"
      },
      "source": [
        "def divisors(n):\n",
        "    \"\"\"Find all divisors of a number\"\"\"\n",
        "    i = 1\n",
        "    divisors = []\n",
        "    while i <= n**0.5:\n",
        "        if (n % i == 0) : \n",
        "            if (n / i == i):\n",
        "                divisors.append(i)\n",
        "            else:\n",
        "                divisors.extend([i, n // i])\n",
        "        i = i + 1\n",
        "    return sorted(divisors)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "giIjs1JOvvvE"
      },
      "source": [
        "def to_matrix(data, token_to_id=None, max_len=None, dtype='int32', batch_first=True):\n",
        "    \"\"\"Casts a list of names into rnn-digestable matrix\n",
        "       parameter token_to_id is None mean that the data is already tokenized\"\"\"\n",
        "    \n",
        "    if token_to_id is None:\n",
        "        data_ix = np.zeros([len(data), max_len], dtype)\n",
        "        for i in range(len(data)):\n",
        "            line_ix = data[i]\n",
        "            data_ix[i, :len(line_ix)] = line_ix\n",
        "    else:\n",
        "        data_ix = np.zeros([len(data), max_len], dtype) + token_to_id[' ']\n",
        "        for i in range(len(data)):\n",
        "            line_ix = [token_to_id[c] for c in data[i]]\n",
        "            data_ix[i, :len(line_ix)] = line_ix\n",
        "        \n",
        "    if not batch_first: # convert [batch, time] into [time, batch]\n",
        "        data_ix = np.transpose(data_ix)\n",
        "\n",
        "    return data_ix\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2QZ4jvNtRoop"
      },
      "source": [
        "class RNN_model(nn.Module):\n",
        "    def __init__(self, model_type, num_tokens, emb_size=16, num_units=64, num_layers=1):\n",
        "        super(self.__class__, self).__init__()\n",
        "        self.emb = nn.Embedding(num_tokens, emb_size)\n",
        "        if model_type == \"VanillaRNN\":\n",
        "            self.rnn = nn.RNN(emb_size, num_units, num_layers, batch_first=True)\n",
        "        elif model_type == 'LSTM':\n",
        "            self.rnn = nn.LSTM(emb_size, num_units, num_layers, batch_first=True)\n",
        "        elif model_type == \"GRU\":\n",
        "            self.rnn = nn.GRU(emb_size, num_units, num_layers, batch_first=True)\n",
        "        self.hid_to_logits = nn.Linear(num_units, num_tokens)\n",
        "\n",
        "    def forward(self, x, hidden=None):  # may pass hidden when generating\n",
        "        if hidden is None:\n",
        "            rnn_out, hidden = self.rnn(self.emb(x))\n",
        "        else:\n",
        "            rnn_out, hidden = self.rnn(self.emb(x), hidden)\n",
        "        logits = self.hid_to_logits(rnn_out)\n",
        "        logits = F.log_softmax(logits, dim=-1)\n",
        "        return logits, hidden\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0-aBS-yD5Crc"
      },
      "source": [
        "def train_eval_loop(model, dataset, vocab=None, lr=1e-3, epoch_n=10, batch_size=32,\n",
        "                    device=None, early_stopping_patience=10, l2_reg_alpha=0, \n",
        "                    optimizer_ctor=None, lr_scheduler_ctor=None, \n",
        "                    draw_loss=False, show_lr=False):\n",
        "\n",
        "    if device is None:\n",
        "        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    device = torch.device(device)\n",
        "    model.to(device)\n",
        "\n",
        "    if optimizer_ctor is None:\n",
        "        optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=l2_reg_alpha)\n",
        "    else:\n",
        "        optimizer = optimizer_ctor(model.parameters(), lr=lr)\n",
        "    \n",
        "    if lr_scheduler_ctor is not None:\n",
        "        lr_scheduler = lr_scheduler_ctor(optimizer)\n",
        "    else:\n",
        "        lr_scheduler = None\n",
        "\n",
        "    best_loss = float(\"inf\")\n",
        "    best_epoch_i = 0\n",
        "    best_model = deepcopy(model)\n",
        "\n",
        "    MAX_LEN = max(map(len, dataset))\n",
        "    \n",
        "    loss_functor = nn.NLLLoss()\n",
        "\n",
        "    for epoch_i in range(epoch_n):\n",
        "        try:\n",
        "            epoch_start = datetime.datetime.now()\n",
        "            print(f\"Epoch {epoch_i}\")\n",
        "\n",
        "            model.train()\n",
        "            mean_loss = 0\n",
        "            batches_n = len(quotes) // batch_size\n",
        "            history = []\n",
        "            for i in tqdm(range(batches_n)):\n",
        "                batch_ix = to_matrix(random.sample(dataset, batch_size), vocab, max_len=MAX_LEN)\n",
        "                batch_ix = torch.tensor(batch_ix, dtype=torch.int64)\n",
        "                batch_ix = copy_data_to_device(batch_ix, device)\n",
        "    \n",
        "                logp_seq, _ = model(batch_ix)\n",
        "                \n",
        "                predictions_logp = logp_seq[:, :-1].reshape(batch_size * (max_len - 1), -1)\n",
        "\n",
        "                actual_next_tokens = batch_ix[:, 1:].reshape(batch_size * (max_len - 1))\n",
        "\n",
        "                loss = loss_functor(predictions_logp, actual_next_tokens)\n",
        "\n",
        "                if draw_loss:\n",
        "                    history.append(loss.data.cpu().numpy())\n",
        "\n",
        "                model.zero_grad()\n",
        "\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "                \n",
        "                mean_loss += float(loss)\n",
        "            \n",
        "            if draw_loss:\n",
        "                plt.plot(history, label=\"loss\")\n",
        "                plt.legend()\n",
        "                plt.show()\n",
        "\n",
        "            mean_loss /= batches_n\n",
        "            print(\"{} iterations, {:0.2f} sec\".format(batches_n,\n",
        "                                                           (datetime.datetime.now() - epoch_start).total_seconds()))\n",
        "            print(f\"Average value of the loss function: {mean_loss}\")\n",
        "\n",
        "            if mean_loss < best_loss:\n",
        "                best_epoch_i = epoch_i\n",
        "                best_loss = mean_loss\n",
        "                best_model = deepcopy(model)\n",
        "                print(\"New best model!\")\n",
        "            elif epoch_i - best_epoch_i > early_stopping_patience:\n",
        "                print(f\"The model has not improved over the last {early_stopping_patience} epochs, stop training\")\n",
        "                break\n",
        "\n",
        "            if lr_scheduler is not None:\n",
        "                if isinstance(lr_scheduler, torch.optim.lr_scheduler.ReduceLROnPlateau):\n",
        "                    lr_scheduler.step(mean_loss)\n",
        "                elif isinstance(lr_scheduler, torch.optim.lr_scheduler.StepLR):\n",
        "                    lr_scheduler.step()\n",
        "                    if show_lr:\n",
        "                        print(optimizer.param_groups[0]['lr'])\n",
        "                else:\n",
        "                    lr_scheduler.step()\n",
        "\n",
        "            print()\n",
        "\n",
        "        except KeyboardInterrupt:\n",
        "            print('Stopped early by user')\n",
        "            break\n",
        "        except Exception as ex:\n",
        "            print('Error while training: {}\\n{}'.format(ex, format_exc()))\n",
        "            break\n",
        "\n",
        "    return best_loss, best_model\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RN2bUYhoZJX2"
      },
      "source": [
        "def generate_sample(model, tokens, token_to_id, max_length, seed_phrase=' ', temperature=1.0, device=None):\n",
        "    '''\n",
        "    The function generates text given a phrase of length at least SEQ_LENGTH.\n",
        "    :param seed_phrase: prefix characters. The RNN is asked to continue the phrase\n",
        "    :param max_length: maximum output length, including seed_phrase\n",
        "    :param temperature: coefficient for sampling.  higher temperature produces more chaotic outputs,\n",
        "                        smaller temperature converges to the single most likely output\n",
        "    '''\n",
        "    \n",
        "    if device is None:\n",
        "        device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "    device = torch.device(device)\n",
        "\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        x_sequence = [[token_to_id[token] for token in seed_phrase]]\n",
        "        x_sequence = torch.tensor([x_sequence], dtype=torch.int64)\n",
        "        x_sequence = copy_data_to_device(x_sequence, device)\n",
        "        \n",
        "        hidden = None\n",
        "\n",
        "        if len(seed_phrase) > 1:\n",
        "            _, hidden = model(torch.squeeze(x_sequence[:, :, :-1], 0))\n",
        "\n",
        "        for _ in range(max_length - len(seed_phrase)):\n",
        "            logp_next, hidden = model(x_sequence[:, :, -1], hidden)\n",
        "            p_next = F.softmax(logp_next / temperature, dim=-1).data.cpu().numpy()[0]\n",
        "            next_ix = np.random.choice(len(tokens), p=p_next[0])\n",
        "            next_ix = torch.tensor([[[next_ix]]], dtype=torch.int64)\n",
        "            x_sequence = copy_data_to_device(torch.cat([x_sequence.cpu(), next_ix], dim=2), device)\n",
        "\n",
        "        return ''.join([tokens[ix] for ix in x_sequence[0, 0].data.cpu().numpy()])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AV8Bhw02i4sc"
      },
      "source": [
        "def generate_sample_bpe(model, tokenizer, max_length, seed_phrase=' ', temperature=1.0, device=None):\n",
        "    '''\n",
        "    The function generates text given a phrase of length at least SEQ_LENGTH.\n",
        "    :param seed_phrase: prefix characters. The RNN is asked to continue the phrase\n",
        "    :param max_length: maximum output length, including seed_phrase\n",
        "    :param temperature: coefficient for sampling.  higher temperature produces more chaotic outputs,\n",
        "                        smaller temperature converges to the single most likely output\n",
        "    '''\n",
        "    \n",
        "    if device is None:\n",
        "        device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "    device = torch.device(device)\n",
        "\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "    \n",
        "    with torch.no_grad():\n",
        "\n",
        "        x_sequence = [tokenizer.encode(seed_phrase, bos=True, eos=False)]\n",
        "        x_sequence = torch.tensor([x_sequence], dtype=torch.int64)\n",
        "        x_sequence = copy_data_to_device(x_sequence, device)\n",
        "        \n",
        "        hidden = None\n",
        "\n",
        "        if len(seed_phrase) > 1:\n",
        "            _, hidden = model(torch.squeeze(x_sequence[:, :, :-1], 0))\n",
        "\n",
        "        length = len(seed_phrase) - 1 # subtrack one because encoding ignore whitespace at the begining\n",
        "        while True:\n",
        "            logp_next, hidden = model(x_sequence[:, :, -1], hidden)\n",
        "            p_next = F.softmax(logp_next / temperature, dim=-1).data.cpu().numpy()[0]\n",
        "            next_ix = np.random.choice(len(tokenizer.vocab()), p=p_next[0])\n",
        "            if next_ix not in [2, 3]:\n",
        "                length += len(tokenizer.vocab()[next_ix])\n",
        "            if length > max_length:\n",
        "                 break\n",
        "            next_ix = torch.tensor([[[next_ix]]], dtype=torch.int64)\n",
        "            x_sequence = copy_data_to_device(torch.cat([x_sequence.cpu(), next_ix], dim=2), device)\n",
        "\n",
        "        return ''.join(tokenizer.decode(x_sequence[0, 0].data.cpu().numpy().tolist(), ignore_ids=[0,2,3])) #ignore <PAD>, <BOS>, <EOS>\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rPtOSQEdx3sH"
      },
      "source": [
        "## Loading data & preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hfPqRIBUwQ9k"
      },
      "source": [
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-11-05T18:21:03.509714Z",
          "start_time": "2019-11-05T18:21:03.491489Z"
        },
        "id": "x9P7lHl7_1W_"
      },
      "source": [
        "dataset_filename = \"/content/gdrive/My Drive/ML/datasets/author_quotes.txt\"\n",
        "with open(dataset_filename) as input_file:\n",
        "    quotes = input_file.read().split('\\n')\n",
        "    quotes = [' ' + line for line in quotes]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-11-05T18:21:03.946758Z",
          "start_time": "2019-11-05T18:21:03.938432Z"
        },
        "id": "7Q_dIWmJ_1W_"
      },
      "source": [
        "quotes_n = len(quotes)\n",
        "print(f\"Number of quotes: {quotes_n}\")\n",
        "quotes[:5]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "owishpHL_1W_"
      },
      "source": [
        "### Quote length distribution:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-11-05T18:21:05.420060Z",
          "start_time": "2019-11-05T18:21:05.179513Z"
        },
        "id": "kG1b0g28_1W_"
      },
      "source": [
        "plt.title('Quote length distribution')\n",
        "plt.hist(list(map(len, quotes)), bins=25);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "em5_JSKm_1W_"
      },
      "source": [
        "### Creating a dictionary {symbol: id}:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H2J_3Inm0UmB"
      },
      "source": [
        "tokens = list(set(''.join(quotes)))\n",
        "token_to_id = {token: idx for idx, token in enumerate(tokens)}\n",
        "num_tokens = len(tokens)\n",
        "\n",
        "print(f\"Number of unique tokens: {num_tokens}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_iPC6dIF3I_t"
      },
      "source": [
        "### Finding the appropriate batch size:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s0EcfUqf3ICj"
      },
      "source": [
        "print(f\"divisors of number of quotes ({quotes_n}) are {divisors(quotes_n)}\")\n",
        "batch_size = 214"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5rvxT91A49Yh"
      },
      "source": [
        "## Byte pair encoding (BPE) tokenization instead of characters using [youtokentome library](https://pypi.org/project/youtokentome/) :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QY_9EDBFFVqf"
      },
      "source": [
        "num_tokens_bpe = 300\n",
        "bpe_model_filename = \"/tmp/quotes_bpe.yttm\"\n",
        "yttm.BPE.train(data=dataset_filename, vocab_size=num_tokens_bpe, model=bpe_model_filename)\n",
        "tokenizer = yttm.BPE(bpe_model_filename)\n",
        "quotes_bpe = tokenizer.encode(quotes, bos=True, eos=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mvYTX51M5v4u"
      },
      "source": [
        "___"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DTxDBv462CaG"
      },
      "source": [
        "## [Character GRU model](#scrollTo=2QZ4jvNtRoop&line=1&uniqifier=1):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uTYYUdas2LMv"
      },
      "source": [
        "char_gru_model = RNN_model(\"GRU\", num_tokens, emb_size=128, num_units=256, num_layers=3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Yh5woVw2W4r"
      },
      "source": [
        "### [Training](#scrollTo=0-aBS-yD5Crc&line=1&uniqifier=1):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7dciemxB2WPV"
      },
      "source": [
        "# scheduler = lambda optim: \\\n",
        "#     torch.optim.lr_scheduler.ReduceLROnPlateau(optim, patience=5, factor=0.5, verbose=True)\n",
        "\n",
        "scheduler = lambda optim: \\\n",
        "    torch.optim.lr_scheduler.StepLR(optim, step_size=5, gamma=0.95)\n",
        "\n",
        "best_char_gru_loss, best_char_gru_model = train_eval_loop(model=char_gru_model,\n",
        "                                                          dataset=quotes,\n",
        "                                                          vocab=token_to_id,\n",
        "                                                          lr=1e-3,\n",
        "                                                          epoch_n=200,\n",
        "                                                          batch_size=batch_size, \n",
        "                                                          early_stopping_patience=10,\n",
        "                                                          l2_reg_alpha=0,\n",
        "                                                          lr_scheduler_ctor=scheduler)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cGhebLuY2Z7y"
      },
      "source": [
        "### [Generating](#scrollTo=RN2bUYhoZJX2&line=1&uniqifier=1) new quotes by character GRU model:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qYVkUvir2dHt"
      },
      "source": [
        "for _ in range(10):\n",
        "    print(generate_sample(best_char_gru_model, tokens, token_to_id, max_length=150, seed_phrase=' Neural networks', temperature=0.5, device='cpu'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HDjhnRt_HC-s"
      },
      "source": [
        "### Could save or load model: "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CptBPDz8tWxd"
      },
      "source": [
        "# model_savepath = \"/tmp/char_gru_model_\" + best_char_gru_loss + \".pth\"\r\n",
        "# torch.save(best_char_gru_model.state_dict(), model_savepath)\r\n",
        "# files.download(model_savepath)\r\n",
        "\r\n",
        "# model_loadpath = \"/content/gdrive/My Drive/ML/models/model_name.pth\" \r\n",
        "# loaded_model = RNN_model(\"GRU\", num_tokens, emb_size=128, num_units=256, num_layers=3)\r\n",
        "# loaded_model.load_state_dict(torch.load(model_loadpath))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZOZnmPYaDy0M"
      },
      "source": [
        "___"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Xr9j-XS6PHU"
      },
      "source": [
        "## [BPE GRU model](#scrollTo=2QZ4jvNtRoop&line=1&uniqifier=1):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tr0qIndFtJK5"
      },
      "source": [
        "bpe_gru_model = RNN_model(\"GRU\", num_tokens_bpe, emb_size=256, num_units=512, num_layers=3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RfiOHO3U6Yqi"
      },
      "source": [
        "### [Training](#scrollTo=0-aBS-yD5Crc&line=1&uniqifier=1):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CUWZTogx2IeR"
      },
      "source": [
        "# scheduler = lambda optim: \\\n",
        "#     torch.optim.lr_scheduler.ReduceLROnPlateau(optim, patience=5, factor=0.5, verbose=True)\n",
        "\n",
        "scheduler = lambda optim: \\\n",
        "    torch.optim.lr_scheduler.StepLR(optim, step_size=5, gamma=0.95)\n",
        "\n",
        "best_bpe_gru_loss, best_bpe_gru_model = train_eval_loop(model=bpe_gru_model,\n",
        "                                                        dataset=quotes_bpe,\n",
        "                                                        lr=1e-3,\n",
        "                                                        epoch_n=150,\n",
        "                                                        batch_size=batch_size, \n",
        "                                                        early_stopping_patience=10,\n",
        "                                                        l2_reg_alpha=0,\n",
        "                                                        lr_scheduler_ctor=scheduler)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BbG-fiLJ6grh"
      },
      "source": [
        "### [Generating](#scrollTo=AV8Bhw02i4sc&line=1&uniqifier=1) new quotes by BPE GRU model:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lHWggtvk6kiQ"
      },
      "source": [
        "for _ in range(10):\n",
        "    print(generate_sample_bpe(best_bpe_gru_model, tokenizer, max_length=150, seed_phrase=' ', temperature=1.0, device='cpu'))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}